# Dynamic Risk Assessment System
The fourth project for [ML DevOps Engineer Nanodegree](https://www.udacity.com/course/machine-learning-dev-ops-engineer-nanodegree--nd0821) by Udacity.

## Description
This project is part of Unit 5: Machine Learning Model Scoring and Monitoring. The problem is to create, deploy, and monitor a risk assessment ML model that will estimate the attrition risk of each of the company's clients. Also setting up processes to re-train, re-deploy, monitor and report on the ML model.

## Prerequisites
- Python 3 required
- Linux environment may be needed within windows through WSL

## Dependencies
This project dependencies is available in the ```requirements.txt``` file.

## Installation
Use the package manager [pip](https://pip.pypa.io/en/stable/) to install the dependencies from the ```requirements.txt```. Its recommended to install it in a separate virtual environment.

```bash
pip install -r requirements.txt
```

## Project Structure
```bash
ðŸ“¦Dynamic-Risk-Assessment-System
 â”£
 â”£ ðŸ“‚data
 â”ƒ â”£ ðŸ“‚ingesteddata                 # Contains csv and metadata of the ingested data
 â”ƒ â”ƒ â”£ ðŸ“œfinaldata.csv
 â”ƒ â”ƒ â”— ðŸ“œingestedfiles.txt
 â”ƒ â”£ ðŸ“‚practicedata                 # Data used for practice mode initially
 â”ƒ â”ƒ â”£ ðŸ“œdataset1.csv
 â”ƒ â”ƒ â”— ðŸ“œdataset2.csv
 â”ƒ â”£ ðŸ“‚sourcedata                   # Data used for production mode
 â”ƒ â”ƒ â”£ ðŸ“œdataset3.csv
 â”ƒ â”ƒ â”— ðŸ“œdataset4.csv
 â”ƒ â”— ðŸ“‚testdata                     # Test data
 â”ƒ â”ƒ â”— ðŸ“œtestdata.csv
 â”£ ðŸ“‚model
 â”ƒ â”£ ðŸ“‚models                       # Models pickle, score, and reports for production mode
 â”ƒ â”ƒ â”£ ðŸ“œapireturns.txt
 â”ƒ â”ƒ â”£ ðŸ“œconfusionmatrix.png
 â”ƒ â”ƒ â”£ ðŸ“œlatestscore.txt
 â”ƒ â”ƒ â”£ ðŸ“œsummary_report.pdf
 â”ƒ â”ƒ â”— ðŸ“œtrainedmodel.pkl
 â”ƒ â”£ ðŸ“‚practicemodels               # Models pickle, score, and reports for practice mode
 â”ƒ â”ƒ â”£ ðŸ“œapireturns.txt
 â”ƒ â”ƒ â”£ ðŸ“œconfusionmatrix.png
 â”ƒ â”ƒ â”£ ðŸ“œlatestscore.txt
 â”ƒ â”ƒ â”£ ðŸ“œsummary_report.pdf
 â”ƒ â”ƒ â”— ðŸ“œtrainedmodel.pkl
 â”ƒ â”— ðŸ“‚production_deployment        # Deployed models and model metadata needed
 â”ƒ â”ƒ â”£ ðŸ“œingestedfiles.txt
 â”ƒ â”ƒ â”£ ðŸ“œlatestscore.txt
 â”ƒ â”ƒ â”— ðŸ“œtrainedmodel.pkl
 â”£ ðŸ“‚src
 â”ƒ â”£ ðŸ“œapicalls.py                  # Runs app endpoints
 â”ƒ â”£ ðŸ“œapp.py                       # Flask app
 â”ƒ â”£ ðŸ“œconfig.py                    # Config file for the project which depends on config.json
 â”ƒ â”£ ðŸ“œdeployment.py                # Model deployment script
 â”ƒ â”£ ðŸ“œdiagnostics.py               # Model diagnostics script
 â”ƒ â”£ ðŸ“œfullprocess.py               # Process automation
 â”ƒ â”£ ðŸ“œingestion.py                 # Data ingestion script
 â”ƒ â”£ ðŸ“œpretty_confusion_matrix.py   # Plots confusion matrix
 â”ƒ â”£ ðŸ“œreporting.py                 # Generates confusion matrix and PDF report
 â”ƒ â”£ ðŸ“œscoring.py                   # Scores trained model
 â”ƒ â”£ ðŸ“œtraining.py                  # Model training
 â”ƒ â”— ðŸ“œwsgi.py
 â”£ ðŸ“œconfig.json                    # Config json file
 â”£ ðŸ“œcronjob.txt                    # Holds cronjob created for automation
 â”£ ðŸ“œREADME.md
 â”— ðŸ“œrequirements.txt               # Projects required dependencies
```

## Steps Overview
1. **Data ingestion:** Automatically check if new data that can be used for model training. Compile all training data to a training dataset and save it to folder. 
2. **Training, scoring, and deploying:** Write scripts that train an ML model that predicts attrition risk, and score the model. Saves the model and the scoring metrics.
3. **Diagnostics:** Determine and save summary statistics related to a dataset. Time the performance of some functions. Check for dependency changes and package updates.
4. **Reporting:** Automatically generate plots and PDF document that report on model metrics and diagnostics. Provide an API endpoint that can return model predictions and metrics.
5. **Process Automation:** Create a script and cron job that automatically run all previous steps at regular intervals.

<img src="images/fullprocess.jpg" width=550 height=300>

## Usage

### 1- Edit config.json file to use practice data

```bash
"input_folder_path": "practicedata",
"output_folder_path": "ingesteddata", 
"test_data_path": "testdata", 
"output_model_path": "practicemodels", 
"prod_deployment_path": "production_deployment"
```

### 2- Run data ingestion
```python
cd src
python ingestion.py
```
Artifacts output:
```
data/ingesteddata/finaldata.csv
data/ingesteddata/ingestedfiles.txt
```

### 3- Model training
```python
python training.py
```
Artifacts output:
```
models/practicemodels/trainedmodel.pkl
```

###  4- Model scoring 
```python
python scoring.py
```
Artifacts output: 
```
models/practicemodels/latestscore.txt
``` 

### 5- Model deployment
```python
python deployment.py
```
Artifacts output:
```
models/prod_deployment_path/ingestedfiles.txt
models/prod_deployment_path/trainedmodel.pkl
models/prod_deployment_path/latestscore.txt
``` 

### 6- Run diagnostics
```python
python diagnostics.py
```

### 7- Run reporting
```python
python reporting.py
```
Artifacts output:
```
models/practicemodels/confusionmatrix.png
models/practicemodels/summary_report.pdf
```

### 8- Run Flask App
```python
python app.py
```

### 9- Run API endpoints
```python
python apicalls.py
```
Artifacts output:
```
models/practicemodels/apireturns.txt
```

### 11- Edit config.json file to use production data

```bash
"input_folder_path": "sourcedata",
"output_folder_path": "ingesteddata", 
"test_data_path": "testdata", 
"output_model_path": "models", 
"prod_deployment_path": "production_deployment"
```

### 10- Full process automation
```python
python fullprocess.py
```
### 11- Cron job

Start cron service
```bash
sudo service cron start
```

Edit crontab file
```bash
sudo crontab -e
```
   - Select **option 3** to edit file using vim text editor
   - Press **i** to insert a cron job
   - Write the cron job in ```cronjob.txt``` which runs ```fullprocces.py``` every 10 mins
   - Save after editing, press **esc key**, then type **:wq** and press enter
  
View crontab file
```bash
sudo crontab -l
```

## License
Distributed under the [MIT](https://choosealicense.com/licenses/mit/) License. See ```LICENSE``` for more information.

## Resources

- Flask
  - https://pythonbasics.org/flask-http-methods/
  - https://www.sqlshack.com/create-rest-apis-in-python-using-flask/
  - https://medium.com/@shanakachathuranga/end-to-end-machine-learning-pipeline-with-mlops-tools-mlflow-dvc-flask-heroku-evidentlyai-github-c38b5233778c

- Reportlab
  - https://www.youtube.com/playlist?list=PLOGAj7tCqHx-IDg2x6cWzqN0um8Z4plQT
  - https://www.reportlab.com/docs/reportlab-userguide.pdf
